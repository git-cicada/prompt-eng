# -*- coding: utf-8 -*-
"""personas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zFClNSVoeoIOu8eVtB9525ZwYP05tZaX

## Personas of Thought
Asking the LLM to generate a crowd of personas to answer a question before aggregating their feedback.

This prompt inspired the new startup I started, [AskRally](https://askrally.com?utm_source=udemy), a virtual audience simulator for synthetic research.
"""

from openai import OpenAI

question = "I'm starting a company that makes shoes that fit any foot size. What do you think of the name 'OmniFit'?"
number = 10

# def call_openai(messages):
#     client = OpenAI()
#     response = client.chat.completions.create(
#         model="gpt-4o-mini",
#         messages=messages,
#         temperature=0.7
#     )
#     return response.choices[0].message.content

import openai

def call_openai(messages):
    client = openai.OpenAI(
    base_url="http://127.0.0.1:1234/v1",  # LM Studio's default local server
    api_key="lm-studio"  # Dummy key, LM Studio doesn't require auth
)
    response = client.chat.completions.create(
        model="llama-3.2-3b-instruct",
        messages=messages,
        temperature=0.7
    )
    return response.choices[0].message.content

system_prompt = "You are a helpful and terse assistant."
question_prompt = "I want a paragraph response to the following question: {question}"

naive_messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": question_prompt.format(question=question)}
]

naive_response = call_openai(naive_messages)

print(naive_response)

experts_prompt = """I want a paragraph response to the following question: {question}

First, name {number} world-class experts (past or present) who would be great at answering this?

Then for each expert, please answer the question critically from their perspective given their background and experience.

Finally, combine the responses into a single response as if these experts had collaborated in writing a joint anonymous answer.

## Expert names:
Expert name (relevant experience)
Expert name (relevant experience)
Expert name (relevant experience)
...

### Expert responses:
Expert name: <response>
Expert name: <response>
Expert name: <response>
...

### Final response:
<joint response>
"""

experts_messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": experts_prompt.format(question=question, number=number)}
]

experts_response = call_openai(experts_messages)

print(experts_response)

personas_prompt = """I want a paragraph response to the following question: {question}

First, name {number} demographic personas who would be relevant for answering this?

Then for each persona, please answer the question critically from their perspective given their background and experience.

Finally, combine the responses into a single response as if these personas had collaborated in writing a joint anonymous answer.

### Persona names:
Persona name (relevant demographics)
Persona name (relevant demographics)
Persona name (relevant demographics)
...

### Persona responses:
Persona name: <response>
Persona name: <response>
Persona name: <response>
...

### Final response:
<joint response>
"""

personas_messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": personas_prompt.format(question=question, number=number)}
]

personas_response = call_openai(personas_messages)

print(personas_response)

def extract_final_response(response):
    return response.split("### Final response:")[1].strip()

experts_final_response = extract_final_response(experts_response)
personas_final_response = extract_final_response(personas_response)

print("Naive: ", naive_response)
print("-"*100)
print("Experts: ", experts_final_response)
print("-"*100)
print("Personas: ", personas_final_response)

judge_prompt = """I want you to compare two responses to the following question: {question}

Here are the two responses:

Response 1:
{response1}

Response 2:
{response2}

Please analyze both responses and choose which one is better. Consider factors like:
- Uniqueness of perspectives
- Nonobvious insights
- Sounds human not like an AI

## Your analysis:
<analysis>

## Your choice:
The better response is: Response <1 or 2>
"""

def judge_responses(response1, response2):
    judge_messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": judge_prompt.format(
            question="How can I be more productive?",
            response1=response1,
            response2=response2
        )}
    ]

    judge_response = call_openai(judge_messages)

    choice = judge_response.split("The better response is: Response ")[1].strip()
    return {"choice": int(choice), "reason": judge_response}


naive_vs_experts = judge_responses(naive_response, experts_final_response)
print("# Naive vs experts:", naive_vs_experts["choice"])

naive_vs_personas = judge_responses(naive_response, personas_final_response)
print("# Naive vs personas:", naive_vs_personas["choice"])

experts_vs_personas = judge_responses(experts_final_response, personas_final_response)
print("# Experts vs personas:", experts_vs_personas["choice"])

print("-"*100)
print(naive_vs_experts)
print("-"*100)
print(naive_vs_personas)
print("-"*100)
print(experts_vs_personas)

"""### Bringing it all together"""

questions = [
    "What is the best way to learn a new skill?",
    "What is the best way to stay healthy?",
    "How can I be more productive?",
    "What will AI look like in 10 years?",
    "How do we end world hunger?",
]

number = 10

def run_test(question):

    naive_messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": question_prompt.format(question=question)}
    ]

    naive_response = call_openai(naive_messages)

    print(naive_response)


    experts_messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": experts_prompt.format(question=question, number=number)}
    ]

    experts_response = call_openai(experts_messages)

    print(experts_response)


    personas_messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": personas_prompt.format(question=question, number=number)}
    ]

    personas_response = call_openai(personas_messages)

    print(personas_response)


    experts_final_response = extract_final_response(experts_response)
    personas_final_response = extract_final_response(personas_response)

    print("Naive: ", naive_response)
    print("-"*100)
    print("Experts: ", experts_final_response)
    print("-"*100)
    print("Personas: ", personas_final_response)

    naive_vs_experts = judge_responses(naive_response, experts_final_response)
    print("# Naive vs experts:", naive_vs_experts["choice"])

    naive_vs_personas = judge_responses(naive_response, personas_final_response)
    print("# Naive vs personas:", naive_vs_personas["choice"])

    experts_vs_personas = judge_responses(experts_final_response, personas_final_response)
    print("# Experts vs personas:", experts_vs_personas["choice"])

    print("-"*100)
    print(naive_vs_experts)
    print("-"*100)
    print(naive_vs_personas)
    print("-"*100)
    print(experts_vs_personas)

    return {
        "naive": naive_response,
        "experts": experts_final_response,
        "personas": personas_final_response,
        "naive_vs_experts": naive_vs_experts,
        "naive_vs_personas": naive_vs_personas,
        "experts_vs_personas": experts_vs_personas,
    }

results = {}

for question in questions:
    results[question] = run_test(question)

# Count wins for each approach
wins = {"naive": 0, "experts": 0, "personas": 0}
trials = {"naive": 0, "experts": 0, "personas": 0}

for question in results:
    # Naive vs experts
    if results[question]["naive_vs_experts"]["choice"] == 1:
        wins["naive"] += 1
    else:
        wins["experts"] += 1
    trials["naive"] += 1
    trials["experts"] += 1

    # Naive vs personas
    if results[question]["naive_vs_personas"]["choice"] == 1:
        wins["naive"] += 1
    else:
        wins["personas"] += 1
    trials["naive"] += 1
    trials["personas"] += 1

    # Experts vs personas
    if results[question]["experts_vs_personas"]["choice"] == 1:
        wins["experts"] += 1
    else:
        wins["personas"] += 1
    trials["experts"] += 1
    trials["personas"] += 1

# Calculate percentages
percentages = {
    approach: (wins[approach] / trials[approach]) * 100
    for approach in wins
}

print("\nWin percentages:")
for approach, percentage in percentages.items():
    print(f"{approach}: {percentage:.1f}%")

